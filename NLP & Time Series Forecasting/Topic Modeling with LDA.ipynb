{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "105a81d6-f7a2-45e8-b3d2-9677cd19b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Gensim imports\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab4cdeb6-aec6-4308-b45e-6fd106c92634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\New\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\New\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14fa8b85-b65d-421c-b24e-8572c42bcdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    file_path = 'data_files/BBC News Train.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset not found!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da15895a-166e-43d4-9970-4b62a3dcbb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ArticleId                                               Text  Category\n",
      "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1        154  german business confidence slides german busin...  business\n",
      "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4        917  enron bosses in $168m payout eighteen former e...  business\n",
      "\n",
      "We have 1490 articles to analyze.\n"
     ]
    }
   ],
   "source": [
    "# We only need the 'Text' column for this\n",
    "if 'df' in locals():\n",
    "    print(df.head())\n",
    "    print(f\"\\nWe have {len(df)} articles to analyze.\")\n",
    "    # Keep only the 'Text' column for our corpus\n",
    "    documents = df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f16487-028d-48ae-9a5d-347973d2d3f5",
   "metadata": {},
   "source": [
    "##### Advanced Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42a6d47b-ba1d-458e-865c-c9e9e7daa8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9b7ec22-c9c4-42a7-a820-39b99ce51a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text_lda(text):\n",
    "    \"\"\"\n",
    "    Prepares text for LDA:\n",
    "    1. Tokenize\n",
    "    2. Lowercase\n",
    "    3. Remove punctuation and numbers\n",
    "    4. Remove stop words\n",
    "    5. Lemmatize\n",
    "    6. Remove short words (<= 3 chars)\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if (token not in stop_words and\n",
    "            token not in string.punctuation and\n",
    "            token.isalpha()): # Remove numbers and punctuation mixes\n",
    "            \n",
    "            lemmatized_token = lemmatizer.lemmatize(token)\n",
    "            \n",
    "            # Remove very short words\n",
    "            if len(lemmatized_token) > 3:\n",
    "                processed_tokens.append(lemmatized_token)\n",
    "                \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac0b9a16-a365-4449-903a-5c3d03074cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting text preprocessing for LDA...\n",
      "Preprocessing complete.\n",
      "\n",
      "--- Example Preprocessed Document ---\n",
      "['worldcom', 'launch', 'defence', 'lawyer', 'defending', 'former', 'worldcom', 'chief', 'bernie', 'ebbers', 'battery', 'fraud', 'charge', 'called', 'company', 'whistleblower', 'first', 'witness', 'cynthia', 'cooper']\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing to all documents\n",
    "if 'documents' in locals():\n",
    "    print(\"\\nStarting text preprocessing for LDA...\")\n",
    "    # This will create a list of lists (each inner list is a doc)\n",
    "    processed_docs = [preprocess_text_lda(doc) for doc in documents]\n",
    "    print(\"Preprocessing complete.\")\n",
    "    \n",
    "    # Print an example\n",
    "    print(\"\\n--- Example Preprocessed Document ---\")\n",
    "    print(processed_docs[0][:20]) # Print first 20 tokens of the first doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3739b89-b643-45d7-a208-ac8c1624785b",
   "metadata": {},
   "source": [
    "##### Create Gensim Corpus and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecd2e0ae-5a2a-48b5-8599-e3b56e20b2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique words (tokens) in dictionary: 5823\n",
      "\n",
      "--- Example Corpus Entry (Doc 0) ---\n",
      "[(0, 7), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "if 'processed_docs' in locals():\n",
    "    # 1. Create the Dictionary\n",
    "    # This maps each word to a unique ID\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    # Filter out extremes (optional but recommended)\n",
    "    # filter words that appear in < 5 documents or > 50% of documents\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    \n",
    "    # 2. Create the Corpus (Bag-of-Words)\n",
    "    # This converts each document into a list of (word_id, frequency)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    \n",
    "    print(f\"\\nNumber of unique words (tokens) in dictionary: {len(dictionary)}\")\n",
    "    \n",
    "    # Print an example of the corpus\n",
    "    print(\"\\n--- Example Corpus Entry (Doc 0) ---\")\n",
    "    print(corpus[0][:10]) # (Word ID, Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f0bf0-4562-4d3c-8c7a-07b2e18bccaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ee62494-4aed-4385-9b9e-2cd973acb495",
   "metadata": {},
   "source": [
    "##### Train the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5793505d-f2cc-4a22-99d7-5531a32ce85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LDA model to find 5 topics...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "if 'corpus' in locals():\n",
    "    # Set number of topics\n",
    "    num_topics = 5\n",
    "    \n",
    "    print(f\"\\nTraining LDA model to find {num_topics} topics...\")\n",
    "    \n",
    "    # Build the LDA model\n",
    "    # `passes` is like epochs\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=100,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        eta='auto'\n",
    "    )\n",
    "    \n",
    "    print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8d795-72d9-4ee3-9add-10b46f8bdc66",
   "metadata": {},
   "source": [
    "##### Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ffd369b-b34d-4636-a8e0-0392b33f4442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Discovered Topics and Key Words ---\n",
      "Topic 0: 0.012*\"people\" + 0.007*\"could\" + 0.005*\"government\" + 0.005*\"many\" + 0.004*\"system\" + 0.004*\"software\" + 0.004*\"user\" + 0.004*\"firm\" + 0.004*\"home\" + 0.004*\"service\"\n",
      "\n",
      "Topic 1: 0.015*\"film\" + 0.010*\"best\" + 0.007*\"award\" + 0.006*\"first\" + 0.006*\"show\" + 0.005*\"star\" + 0.005*\"last\" + 0.005*\"music\" + 0.005*\"world\" + 0.005*\"number\"\n",
      "\n",
      "Topic 2: 0.018*\"game\" + 0.007*\"player\" + 0.007*\"first\" + 0.006*\"time\" + 0.006*\"england\" + 0.005*\"gadget\" + 0.004*\"world\" + 0.004*\"wale\" + 0.004*\"back\" + 0.004*\"technology\"\n",
      "\n",
      "Topic 3: 0.009*\"election\" + 0.009*\"government\" + 0.008*\"labour\" + 0.008*\"party\" + 0.006*\"economy\" + 0.006*\"blair\" + 0.005*\"minister\" + 0.005*\"country\" + 0.005*\"tory\" + 0.005*\"growth\"\n",
      "\n",
      "Topic 4: 0.008*\"mobile\" + 0.008*\"firm\" + 0.008*\"phone\" + 0.007*\"company\" + 0.005*\"service\" + 0.005*\"club\" + 0.005*\"people\" + 0.005*\"sale\" + 0.004*\"share\" + 0.004*\"market\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'lda_model' in locals():\n",
    "    print(\"\\n--- Discovered Topics and Key Words ---\")\n",
    "    \n",
    "    # Get and print the topics\n",
    "    topics = lda_model.print_topics(num_words=10) # Show top 10 words per topic\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(f\"Topic {topic[0]}: {topic[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7bbc1-e829-4a87-9c1d-4e9b8c00823a",
   "metadata": {},
   "source": [
    "##### Interpetation \n",
    "\n",
    "By looking at the keywords, we can manually label these topics:\n",
    "\n",
    "Topic 0 is clearly about \"Sports\".\n",
    "\n",
    "Topic 1 is clearly about \"Entertainment\".\n",
    "\n",
    "Topic 2 is clearly about \"Politics\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77e08d-ef49-4486-91f6-4127e12e492e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00697b7b-e2fd-44c3-9b85-3afdda0ac2ab",
   "metadata": {},
   "source": [
    "##### Validate with Real Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d31e346-5fb1-4aa7-aceb-4934363143ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation: Original Category vs. Discovered Topic ---\n",
      "Discovered_Topic     0    1    2    3    4\n",
      "Original_Category                         \n",
      "business            29    0    2  151  154\n",
      "entertainment       16  251    0    3    3\n",
      "politics            88    2    3  177    4\n",
      "sport                0   91  123   11  121\n",
      "tech               138    3   61    1   58\n"
     ]
    }
   ],
   "source": [
    "if 'lda_model' in locals():\n",
    "    # Create a new DataFrame for analysis\n",
    "    df_results = pd.DataFrame()\n",
    "    df_results['Original_Category'] = df['Category']\n",
    "    \n",
    "    # Get the dominant topic for each document\n",
    "    doc_topics = [lda_model.get_document_topics(doc) for doc in corpus]\n",
    "    \n",
    "    # Find the topic with the highest probability\n",
    "    dominant_topic = []\n",
    "    for doc in doc_topics:\n",
    "        # Sort topics by probability and get the ID of the top one\n",
    "        top_topic = sorted(doc, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topic.append(top_topic)\n",
    "        \n",
    "    df_results['Discovered_Topic'] = dominant_topic\n",
    "    \n",
    "    print(\"\\n--- Validation: Original Category vs. Discovered Topic ---\")\n",
    "    \n",
    "    # Create a cross-tabulation to see the relationship\n",
    "    crosstab = pd.crosstab(df_results['Original_Category'], df_results['Discovered_Topic'])\n",
    "    print(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5efd9-c57e-44e0-b7ef-edcdfc603f66",
   "metadata": {},
   "source": [
    "##### This proves the model successfully discovered the underlying topics without ever being told what they were!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a720e4-c2f2-41ba-afbc-935bf10a4da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
