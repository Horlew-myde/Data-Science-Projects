{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca0a6c8-dc28-4deb-87aa-f8345a52f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8351aa7a-01be-4e83-9674-b443a4e95161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c7ee25c-9c8b-49ef-bf2b-baab48b0cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# You may need to download NLTK data first\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    file_path = 'data_files/spam.csv'\n",
    "    df = pd.read_csv(file_path, encoding='latin-1')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'spam.csv' not found!!\")\n",
    "    # You could add code here to exit or handle the error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9d8df4d-5f1d-4180-b4ce-fe9156317ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "\n",
      "Dataset shape: (5572, 2)\n",
      "\n",
      "Spam vs. Ham counts:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning: Drop unnecessary columns and rename\n",
    "if 'df' in locals():\n",
    "    df = df[['v1', 'v2']]\n",
    "    df.columns = ['label', 'message']\n",
    "    \n",
    "    print(df.head())\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nSpam vs. Ham counts:\")\n",
    "    print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bcc4e-0b2c-422e-823f-b35df139747d",
   "metadata": {},
   "source": [
    "##### Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e5e3fb-34f5-443b-8caf-dc2128688c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and stop words\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b03a1d1a-0262-41d5-8bfc-cb5260d615f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses text data.\n",
    "    1. Lowercase\n",
    "    2. Remove punctuation\n",
    "    3. Remove stop words\n",
    "    4. Apply stemming\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # 3. Tokenize (split into words) and remove stop words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # 4. Apply stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "439390de-54e5-4f86-9d18-2b5042279d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text... (This may take a moment)\n",
      "Preprocessing complete.\n",
      "                                             message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                     cleaned_message  \n",
      "0  go jurong point crazi avail bugi n great world...  \n",
      "1                              ok lar joke wif u oni  \n",
      "2  free entri 2 wkli comp win fa cup final tkt 21...  \n",
      "3                u dun say earli hor u c alreadi say  \n",
      "4          nah dont think goe usf live around though  \n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing to the message column\n",
    "if 'df' in locals():\n",
    "    print(\"\\nPreprocessing text... (This may take a moment)\")\n",
    "    df['cleaned_message'] = df['message'].apply(preprocess_text)\n",
    "    print(\"Preprocessing complete.\")\n",
    "    print(df[['message', 'cleaned_message']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b88ad-63ab-4264-a399-171646ba111b",
   "metadata": {},
   "source": [
    "##### Map Labels and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1984f0ac-a445-4032-a9c8-fe87e2166949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data shape: (4457,)\n",
      "Testing data shape: (1115,)\n"
     ]
    }
   ],
   "source": [
    "if 'df' in locals():\n",
    "    # Map labels to numbers\n",
    "    df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "    \n",
    "    # Define our features (X) and target (y)\n",
    "    X = df['cleaned_message']\n",
    "    y = df['label_num']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "    print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c08f730-9511-484b-9e85-a21662df0868",
   "metadata": {},
   "source": [
    "##### Vectorization (Bag-of-Words vs. TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29ded88f-73ea-46de-ba3c-c46899e53fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BoW Vectorized Training Data Shape: (4457, 7135)\n"
     ]
    }
   ],
   "source": [
    "if 'df' in locals():\n",
    "    # 1. Initialize CountVectorizer\n",
    "    bow_vectorizer = CountVectorizer()\n",
    "    \n",
    "    # 2. Fit and transform the training data\n",
    "    X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # 3. Only transform the testing data (using the vocab from training)\n",
    "    X_test_bow = bow_vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nBoW Vectorized Training Data Shape: {X_train_bow.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29b6f8b2-018b-4a3b-862e-acc9a32b1574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Training Data Shape: (4457, 7135)\n"
     ]
    }
   ],
   "source": [
    "if 'df' in locals():\n",
    "    # 1. Initialize TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # 2. Fit and transform the training data\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # 3. Only transform the testing data\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"TF-IDF Vectorized Training Data Shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635742e-9176-45a0-bc5c-8c3d1662c24d",
   "metadata": {},
   "source": [
    "##### Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "240fc598-0011-4a3e-af6c-3ff95ee0717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test, method_name):\n",
    "    \"\"\"\n",
    "    Trains a Multinomial Naive Bayes model and prints its evaluation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Model Evaluation for: {method_name} ---\")\n",
    "    \n",
    "    # 1. Train the model\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 2. Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 3. Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, target_names=['Ham (0)', 'Spam (1)'])\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3bc0cdf-39cd-487e-b56a-addb0fe940a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation for: Bag-of-Words (BoW) ---\n",
      "Accuracy: 0.9830\n",
      "\n",
      "Confusion Matrix:\n",
      "[[962   4]\n",
      " [ 15 134]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Ham (0)       0.98      1.00      0.99       966\n",
      "    Spam (1)       0.97      0.90      0.93       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "\n",
      "--- Model Evaluation for: TF-IDF ---\n",
      "Accuracy: 0.9623\n",
      "\n",
      "Confusion Matrix:\n",
      "[[965   1]\n",
      " [ 41 108]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Ham (0)       0.96      1.00      0.98       966\n",
      "    Spam (1)       0.99      0.72      0.84       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.98      0.86      0.91      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'df' in locals():\n",
    "    # Train and evaluate BoW model\n",
    "    model_bow = train_and_evaluate(X_train_bow, y_train, X_test_bow, y_test, \"Bag-of-Words (BoW)\")\n",
    "    \n",
    "    # Train and evaluate TF-IDF model\n",
    "    model_tfidf = train_and_evaluate(X_train_tfidf, y_train, X_test_tfidf, y_test, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1340a-ca72-4909-ae35-c925be6f3a68",
   "metadata": {},
   "source": [
    "Confusion Matrix: Shows (Ham, Spam) x (Predicted Ham, Predicted Spam). Look at the false positives (predicted Spam, but was Ham) and false negatives (predicted Ham, but was Spam).\n",
    "\n",
    "Precision (Spam): Of all messages we labeled as spam, what percentage was actually spam? (High precision is good = we don't annoy users by flagging good mail as spam).\n",
    "\n",
    "Recall (Spam): Of all actual spam messages, what percentage did we catch? (High recall is good = we protect users from spam).\n",
    "\n",
    "F1-Score: The harmonic mean of Precision and Recall. A good single metric for an imbalanced dataset like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7681a89-73d9-44b3-a8ff-484e88bd5e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708c0a97-487d-4438-8fb2-1cb239d112a4",
   "metadata": {},
   "source": [
    "##### Test Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24744841-4747-4877-b056-d243220694d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spam(message, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Predicts if a single new message is spam or ham.\n",
    "    \"\"\"\n",
    "    # 1. Preprocess the message\n",
    "    cleaned_message = preprocess_text(message)\n",
    "    \n",
    "    # 2. Vectorize the message\n",
    "    message_vec = vectorizer.transform([cleaned_message])\n",
    "    \n",
    "    # 3. Make a prediction\n",
    "    prediction = model.predict(message_vec)\n",
    "    \n",
    "    # 4. Return the result\n",
    "    return \"Spam\" if prediction[0] == 1 else \"Ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02024285-662b-4961-a067-3eff4d307ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing new messages with TF-IDF model:\n",
      "Message: 'Congratulations! You've won a $1000 gift card. Click here to claim: www.fake.com' \n",
      "Prediction: Spam\n",
      "\n",
      "Message: 'Hey, are you free for dinner tonight at 7?' \n",
      "Prediction: Ham\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage (run this after training 'model_tfidf') ---\n",
    "if 'model_tfidf' in locals():\n",
    "    test_msg_1 = \"Congratulations! You've won a $1000 gift card. Click here to claim: www.fake.com\"\n",
    "    test_msg_2 = \"Hey, are you free for dinner tonight at 7?\"\n",
    "    \n",
    "    print(f\"\\nTesting new messages with TF-IDF model:\")\n",
    "    print(f\"Message: '{test_msg_1}' \\nPrediction: {predict_spam(test_msg_1, tfidf_vectorizer, model_tfidf)}\")\n",
    "    print(f\"\\nMessage: '{test_msg_2}' \\nPrediction: {predict_spam(test_msg_2, tfidf_vectorizer, model_tfidf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e0417-c4d4-4805-8525-a829585e504e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
